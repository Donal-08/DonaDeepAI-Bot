{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c857da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import csv\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea5cd6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e988d",
   "metadata": {},
   "source": [
    "## 1. Visualise the first few lines of the tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc619e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!']\n",
      "['L1044', 'u2', 'm0', 'CAMERON', 'They do to!']\n",
      "['L985', 'u0', 'm0', 'BIANCA', 'I hope so.']\n",
      "['L984', 'u2', 'm0', 'CAMERON', 'She okay?']\n",
      "['L925', 'u0', 'm0', 'BIANCA', \"Let's go.\"]\n",
      "['L924', 'u2', 'm0', 'CAMERON', 'Wow']\n",
      "['L872', 'u0', 'm0', 'BIANCA', \"Okay -- you're gonna need to learn how to lie.\"]\n",
      "['L871', 'u2', 'm0', 'CAMERON', 'No']\n",
      "['\"L870', 'u0', 'm0', 'BIANCA', 'I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"']\n",
      "['L869', 'u0', 'm0', 'BIANCA', 'Like my fear of wearing pastels?']\n",
      "\n",
      "\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "movie_lines_filename = \"cornell_movie_corpus/movie_lines.tsv\"\n",
    "lines_to_visualise = 10\n",
    "\n",
    "# Open the TSV file in read mode\n",
    "with open(movie_lines_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    # Iterate over each line in the file\n",
    "    for i, line in enumerate(file):\n",
    "        # Remove any leading/trailing whitespace and split the line by tabs\n",
    "        row = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # Process the row as desired\n",
    "        print(row)\n",
    "        \n",
    "        # Check if the desired number of lines has been reached\n",
    "        if i + 1 >= lines_to_visualise:\n",
    "            break\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(type(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a96da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u0', 'u2', 'm0', \"['L194' 'L195' 'L196' 'L197']\"]\n",
      "['u0', 'u2', 'm0', \"['L198' 'L199']\"]\n",
      "['u0', 'u2', 'm0', \"['L200' 'L201' 'L202' 'L203']\"]\n",
      "['u0', 'u2', 'm0', \"['L204' 'L205' 'L206']\"]\n",
      "['u0', 'u2', 'm0', \"['L207' 'L208']\"]\n",
      "['u0', 'u2', 'm0', \"['L271' 'L272' 'L273' 'L274' 'L275']\"]\n",
      "['u0', 'u2', 'm0', \"['L276' 'L277']\"]\n",
      "['u0', 'u2', 'm0', \"['L280' 'L281']\"]\n",
      "['u0', 'u2', 'm0', \"['L363' 'L364']\"]\n",
      "['u0', 'u2', 'm0', \"['L365' 'L366']\"]\n",
      "\n",
      "\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "movie_conv_filename = \"cornell_movie_corpus/movie_conversations.tsv\"\n",
    "lines_to_visualise = 10\n",
    "\n",
    "# Open the TSV file in read mode\n",
    "with open(movie_conv_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    # Iterate over each line in the file\n",
    "    for i, line in enumerate(file):\n",
    "        # Remove any leading/trailing whitespace and split the line by tabs\n",
    "        row = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # Process the row as desired\n",
    "        print(row)\n",
    "        \n",
    "        # Check if the desired number of lines has been reached\n",
    "        if i + 1 >= lines_to_visualise:\n",
    "            break\n",
    "     \n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(type(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639a967",
   "metadata": {},
   "source": [
    "## 2. Create relevant dictionaries based on the tsv file datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35b9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = {}\n",
    "movie_lines_fields = ['line ID','user ID','movie ID', 'char name', 'text']\n",
    "\n",
    "# Open the TSV file in read mode\n",
    "with open(movie_lines_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        \n",
    "        # Remove any leading/trailing whitespace and split the line by tabs\n",
    "        line_parts = line.strip().split(\"\\t\")\n",
    "        \n",
    "        if(len(line_parts) > 4):             # Handling the bad datas where there are empty texts\n",
    "            \n",
    "            # Extract the individual parts\n",
    "            line_id = line_parts[0]\n",
    "            user_id = line_parts[1]\n",
    "            movie_id = line_parts[2]\n",
    "            character_name = line_parts[3]\n",
    "            text = line_parts[4]\n",
    "\n",
    "        # Create a dictionary for the line\n",
    "        line_dict = {\n",
    "            'lineID': line_id,\n",
    "            'userID': user_id,\n",
    "            'movieID': movie_id,\n",
    "            'charName': character_name,\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "        # Add the line dictionary to the result dictionary\n",
    "        movie_lines[line_id] = line_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3d8bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 {'lineID': 'L1045', 'userID': 'u0', 'movieID': 'm0', 'charName': 'BIANCA', 'text': 'They do not!'}\n",
      "L1044 {'lineID': 'L1044', 'userID': 'u2', 'movieID': 'm0', 'charName': 'CAMERON', 'text': 'They do to!'}\n",
      "L985 {'lineID': 'L985', 'userID': 'u0', 'movieID': 'm0', 'charName': 'BIANCA', 'text': 'I hope so.'}\n"
     ]
    }
   ],
   "source": [
    "# movie_lines is a dictionary of dictionaries\n",
    "# Print the first 3 of them\n",
    "movie_lines\n",
    "\n",
    "count = 0\n",
    "for key, value in movie_lines.items():\n",
    "    print(key, value)\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb39a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('L1045',\n",
       " {'lineID': 'L1045',\n",
       "  'userID': 'u0',\n",
       "  'movieID': 'm0',\n",
       "  'charName': 'BIANCA',\n",
       "  'text': 'They do not!'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First element of the movie_lines dictionary\n",
    "list(movie_lines.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bc5cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L194', 'L195', 'L196', 'L197']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating the regex function used\n",
    "import re\n",
    "\n",
    "line_numbers_str = \"['L194' 'L195' 'L196' 'L197']\"\n",
    "\n",
    "# Extract the line numbers using regular expressions\n",
    "line_numbers_list = re.findall(r\"'(\\w+)'\", line_numbers_str)\n",
    "\n",
    "print(line_numbers_list)\n",
    "print(type(line_numbers_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79408b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conv_fields = ['charID1','charID2','movieID', 'lineIDs']\n",
    "conversations = []\n",
    "\n",
    "# Open the TSV file in read mode\n",
    "with open(movie_conv_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for i, line in enumerate(file):\n",
    "        conv_dict = {}                            # Declare an empty dictionary\n",
    "        row = line.strip().split(\"\\t\")            #  ['u0', 'u2', 'm0', \"['L194' 'L195' 'L196' 'L197']\"]\n",
    "        for j, conv_field in enumerate(movie_conv_fields):\n",
    "            if(conv_field == 'lineIDs'):\n",
    "                row[j] = re.findall(r\"'(\\w+)'\", row[j])   # matches any alphanumeric characters (\\w+) enclosed in single quotes\n",
    "            conv_dict[conv_field] = row[j] \n",
    "        \n",
    "        conversations.append(conv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597b289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'charID1': 'u0',\n",
       " 'charID2': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'lineIDs': ['L194', 'L195', 'L196', 'L197']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversations is a list of dictionaries\n",
    "# First element of the conversations list\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2cdd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'charID1': 'u0', 'charID2': 'u2', 'movieID': 'm0', 'lineIDs': ['L194', 'L195', 'L196', 'L197'], 'lines': [{'lineID': 'L194', 'userID': 'u0', 'movieID': 'm0', 'charName': 'BIANCA', 'text': 'They do not!'}, {'lineID': 'L195', 'userID': 'u2', 'movieID': 'm0', 'charName': 'CAMERON', 'text': 'They do to!'}]}]\n"
     ]
    }
   ],
   "source": [
    "c = [\n",
    "    {\n",
    "        'charID1': 'u0',\n",
    "        'charID2': 'u2',\n",
    "        'movieID': 'm0',\n",
    "        'lineIDs': ['L194', 'L195', 'L196', 'L197']\n",
    "    },\n",
    "    # other dictionaries\n",
    "    ]\n",
    "\n",
    "m_lines = {\n",
    "    'L194': {'lineID': 'L194', 'userID': 'u0', 'movieID': 'm0', 'charName': 'BIANCA', 'text': 'They do not!'},\n",
    "    'L195': {'lineID': 'L195', 'userID': 'u2', 'movieID': 'm0', 'charName': 'CAMERON', 'text': 'They do to!'},\n",
    "    'L500': {'lineID': 'L500', 'userID': 'u2', 'movieID': 'm0', 'charName': 'CAMERON', 'text': 'They do to!'},\n",
    "    # other dictionaries\n",
    "}\n",
    "\n",
    "for item in c:\n",
    "    line_ids = item['lineIDs']\n",
    "    lines = []\n",
    "    for line_id in line_ids:\n",
    "        line = m_lines.get(line_id)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "    item['lines'] = lines\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c0b45",
   "metadata": {},
   "source": [
    "## 3. Merging the processed conversations list and movie_lines dictionary \n",
    "- Add a new key 'lines' in each dictionary of the conversations list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee68001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv in conversations:\n",
    "    line_ids = conv['lineIDs']\n",
    "    lines = []\n",
    "    for line_id in line_ids:\n",
    "        line = movie_lines.get(line_id)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "    conv['lines'] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267882f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'charID1': 'u0',\n",
       " 'charID2': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'lineIDs': ['L194', 'L195', 'L196', 'L197'],\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'userID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'charName': 'BIANCA',\n",
       "   'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'},\n",
       "  {'lineID': 'L195',\n",
       "   'userID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'charName': 'CAMERON',\n",
       "   'text': \"Well I thought we'd start with pronunciation if that's okay with you.\"},\n",
       "  {'lineID': 'L196',\n",
       "   'userID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'charName': 'BIANCA',\n",
       "   'text': 'Not the hacking and gagging and spitting part.  Please.'},\n",
       "  {'lineID': 'L197',\n",
       "   'userID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'charName': 'CAMERON',\n",
       "   'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671392d5",
   "metadata": {},
   "source": [
    "## 4. Extract q/a pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ea35a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217150"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for conv in conversations:\n",
    "    lines = conv['lines']\n",
    "    for i in range(len(lines) - 1):\n",
    "        q = lines[i]['text'].strip()       # remove trailing and leading whitespace characters \n",
    "        a = lines[i+1]['text'].strip()\n",
    "        if len(q) > 0 and len(a) > 0:       # filter empty lists\n",
    "            qa_pairs.append([q, a])\n",
    "\n",
    "        \n",
    "len(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254a0d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       "  \"Well I thought we'd start with pronunciation if that's okay with you.\"],\n",
       " [\"Well I thought we'd start with pronunciation if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.'],\n",
       " ['Not the hacking and gagging and spitting part.  Please.',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa_pairs being a list of lists, we first visualise the first 3\n",
    "\n",
    "qa_pairs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28d2b5",
   "metadata": {},
   "source": [
    "## 5. Writing a new tsv file using the qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bfd0f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting a newly formatted file .....\n",
      "File written successfully.\n"
     ]
    }
   ],
   "source": [
    "filename = \"cornell_movie_corpus/formatted_qa_pairs.tsv\"\n",
    "\n",
    "print(\"Writting a newly formatted file .....\")\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    for pair in qa_pairs:\n",
    "        q, a = pair\n",
    "        file.write(f\"{q}\\t{a}\\n\")     # format the QnA with a TAB character b/w and a newline b/w every Qna pair \n",
    "\n",
    "print(\"File written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf15ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell I thought we'd start with pronunciation if that's okay with you.\\r\\n\"\n",
      "b\"Well I thought we'd start with pronunciation if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
      "b\"No no it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "with open(filename, \"rb\") as file:\n",
    "    lines = file.readlines()\n",
    "for line in lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec88f17",
   "metadata": {},
   "source": [
    "## 6. Processing the words\n",
    "- Defining a WordIndexer class\n",
    "- Define helper functions to preprocess the text such as - convert unicode to ASCII, normalise the strings\n",
    "- Read from the formatted qa pairs tsv file and store the preprocessed qa pairs in pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d1c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining tokens\n",
    "PAD_token = 0             # for padding short sentences\n",
    "SOS_token = 1             # start of sentence token\n",
    "EOS_token = 2             # End of sentence\n",
    "\n",
    "class WordIndexer:\n",
    "    def __init__(self, corpus_name):\n",
    "        \"\"\"\n",
    "        Initializes a WordIndexer object.\n",
    "\n",
    "        Args:\n",
    "        - corpus_name: A string representing the name of the corpus or dataset.\n",
    "                                                                                        \"\"\"\n",
    "        self.corpus_name = corpus_name\n",
    "        self.special_tokens = ['PAD', 'SOS', 'EOS']\n",
    "        self.word_to_index = {'PAD' : PAD_token, 'SOS': SOS_token, 'EOS': EOS_token}\n",
    "        self.word_counts = {}           # Stores the count of each word in the corpus\n",
    "        self.index_to_word = {PAD_token:'PAD', SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.num_words = 3             # to include the 3 special tokens: PAD, SOS, EOS\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Adds a word to the vocabulary/WordIndexer object\n",
    "\n",
    "        Args:\n",
    "        - word: A string representing the word to be added.\n",
    "                                                                    \"\"\"\n",
    "        if word not in self.word_to_index:\n",
    "            index = len(self.word_to_index) + 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.index_to_word[index] = word\n",
    "            self.word_counts[word] = 1\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word_counts[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            Adds all words in a sentence to the vocabulary/WordIndexer object\n",
    "\n",
    "        Args:\n",
    "        - sentence: A string representing the sentence.\n",
    "                                                                \"\"\"\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "    \n",
    "\n",
    "    def trim_less_freq_words(self, threshold):\n",
    "        \"\"\"\n",
    "        Remove words below a certain count threshold and update the word-index mapping.\n",
    "\n",
    "        Args:\n",
    "        - threshold: int : the minimum count for a word to be retained.\n",
    "        \"\"\"\n",
    "        words_to_remove = []\n",
    "        for word, count in self.word_counts.items():\n",
    "            if count < threshold:\n",
    "                words_to_remove.append(word)\n",
    "\n",
    "        new_word_to_index = {'PAD': PAD_token, 'SOS': SOS_token, 'EOS': EOS_token}\n",
    "        new_index_to_word = {PAD_token: 'PAD', SOS_token: 'SOS', EOS_token: 'EOS'}\n",
    "        new_word_counts = {}\n",
    "        new_num_words = 3                    # to include the 3 special tokens: PAD, SOS, EOS\n",
    "\n",
    "        next_index = 3\n",
    "        for word, index in self.word_to_index.items():\n",
    "            if word not in words_to_remove:\n",
    "                if word not in self.special_tokens:\n",
    "                    new_word_to_index[word] = next_index\n",
    "                    new_index_to_word[next_index] = word\n",
    "                    new_word_counts[word] = self.word_counts[word]\n",
    "                    next_index += 1\n",
    "                    new_num_words += 1\n",
    "\n",
    "        self.word_to_index = new_word_to_index\n",
    "        self.index_to_word = new_index_to_word\n",
    "        self.word_counts = new_word_counts\n",
    "        self.num_words = new_num_words\n",
    "\n",
    "\n",
    "    # Retrieve the index of a word from the vocabulary\n",
    "    def get_word_index(self, word):  \n",
    "        return self.word_to_index.get(word)\n",
    "\n",
    "    # Retruns the word corresponding to an index from the vocabulary\n",
    "    def get_index_word(self, index):\n",
    "        return self.index_to_word.get(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f1640",
   "metadata": {},
   "source": [
    "### A sample Run to test/show the working of the WordIndexer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12782279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SOS': 1,\n",
       " 'EOS': 2,\n",
       " 'My': 4,\n",
       " 'name': 5,\n",
       " 'is': 6,\n",
       " 'Donal.': 7,\n",
       " 'The': 8,\n",
       " 'Sherlock': 9,\n",
       " 'Holmes': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = WordIndexer(\"sample\")\n",
    "W.add_sentence(\"My name is Donal. The name is Sherlock Holmes\")\n",
    "W.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31df08cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My': 1,\n",
       " 'name': 2,\n",
       " 'is': 2,\n",
       " 'Donal.': 1,\n",
       " 'The': 1,\n",
       " 'Sherlock': 1,\n",
       " 'Holmes': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f1b9d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'SOS',\n",
       " 2: 'EOS',\n",
       " 4: 'My',\n",
       " 5: 'name',\n",
       " 6: 'is',\n",
       " 7: 'Donal.',\n",
       " 8: 'The',\n",
       " 9: 'Sherlock',\n",
       " 10: 'Holmes'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548ebd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6773a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.trim_less_freq_words(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855f9e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0, 'SOS': 1, 'EOS': 2, 'name': 3, 'is': 4}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b49b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eea8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "# Thanks to stackoverflow \"https://stackoverflow.com/a/518232/2809427\"\n",
    "def unicode_to_ascii(text):\n",
    "    \"\"\"\n",
    "    Convert Unicode text to ASCII by transliterating non-ASCII characters to their closest ASCII equivalents.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The Unicode text to convert.\n",
    "    Returns:\n",
    "        str: The converted ASCII text.\n",
    "                                                                                                                \"\"\"\n",
    "    return unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f5feba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bei Jing '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_to_ascii('北亰')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1d1b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Francois'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_to_ascii('François')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "502ad286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kozuscek'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_to_ascii('kožušček')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaf7fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(text):\n",
    "    \"\"\"\n",
    "    Normalize a string by converting it to lowercase, adding space before punctuation marks,\n",
    "    removing non-letter characters, and removing sequences of whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        str: The normalized string.\n",
    "    \"\"\"\n",
    "    # Convert the text to lowercase\n",
    "    normalized_text = unicode_to_ascii(text.lower())         # no need to strip()\n",
    "\n",
    "    # Add space before punctuation marks\n",
    "    normalized_text = re.sub(r\"([.,!?])\", r\" \\1\", normalized_text)\n",
    "\n",
    "    # Remove non-letter characters\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z.,!? ]\", \"\", normalized_text)\n",
    "\n",
    "    # Remove sequences of whitespace\n",
    "    normalized_text = re.sub(r\"\\s+\", \" \", normalized_text)\n",
    "\n",
    "    return normalized_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3c8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcaa !ss dd ?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_string(\"    AbC123aa!s's    dd?    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d533fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_file(file_path):\n",
    "    data = []\n",
    "    \n",
    "    print(\"Reading and processing file ...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as tsv_file:\n",
    "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) == 2:\n",
    "                q = normalize_string(row[0])\n",
    "                a = normalize_string(row[1])\n",
    "                data.append([q, a])\n",
    "    print(\"Done reading.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "120ad0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing file ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading.\n"
     ]
    }
   ],
   "source": [
    "data_filename = \"cornell_movie_corpus/formatted_qa_pairs.tsv\"\n",
    "pairs = read_tsv_file(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "554127ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
       "  'well i thought wed start with pronunciation if thats okay with you .'],\n",
       " ['well i thought wed start with pronunciation if thats okay with you .',\n",
       "  'not the hacking and gagging and spitting part . please .']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise the first 2 pairs\n",
    "pairs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81bb0aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217150"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb07b5",
   "metadata": {},
   "source": [
    "## Filtering the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebf82e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_qa_pairs(data, q_threshold=12, a_threshold=12):\n",
    "    \"\"\"\n",
    "    Filters the question-answer pairs beyond a threshold length of words,\n",
    "    Args:\n",
    "    data = (the list of lists containing the normalized question-answer pairs) \n",
    "    q_threshold, a_threshold = (the maximum number of words allowed for the question and answer).\n",
    "                                                                                                        \"\"\"\n",
    "    filtered_data = []\n",
    "    for pair in data:\n",
    "        q = pair[0]\n",
    "        a = pair[1]\n",
    "        if len(q.split()) <= q_threshold and len(a.split()) <= a_threshold:\n",
    "            filtered_data.append(pair)\n",
    "    \n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09159b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 217150 pairs/conversations in the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering (by threshold) , there are 100847 pairs/conversations\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(pairs)} pairs/conversations in the dataset\")\n",
    "filtered_pairs = filter_qa_pairs(pairs)\n",
    "print(f\"After filtering (by threshold) , there are {len(filtered_pairs)} pairs/conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2ed17",
   "metadata": {},
   "source": [
    "### Instantiate an object of the class WordIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b28f4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = WordIndexer(\"cornell_movie_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "999dc64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of words in the voc = 28151\n",
      "['no no its my fault we didnt have a proper introduction', 'cameron .']\n",
      "['gosh if only we could find kat a boyfriend . . .', 'let me see what i can do .']\n",
      "['cesc ma tete . this is my head', 'right . see ? youre ready for the quiz .']\n",
      "['thats because its such a nice one .', 'forget french .']\n",
      "['how is our little find the wench a date plan progressing ?', 'well theres someone i think might be']\n"
     ]
    }
   ],
   "source": [
    "for pair in filtered_pairs:\n",
    "    q, a = pair[0], pair[1]\n",
    "    voc.add_sentence(q)\n",
    "    voc.add_sentence(a)\n",
    "\n",
    "print(f\"Count of words in the voc = {voc.num_words}\")\n",
    "for pair in filtered_pairs[:5]:\n",
    "    print(pair)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eadbc",
   "metadata": {},
   "source": [
    "## Remove those qa pairs if any word of  'q' or 'a' occurs less than a threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63694967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_word_frequency(voc, qa_pairs, threshold):\n",
    "\n",
    "    # Remove words below the threshold from the class instance\n",
    "    voc.trim_less_freq_words(threshold)\n",
    "\n",
    "    # Filter QA pairs based on word frequency\n",
    "    keep_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        q, a = pair[0], pair[1]\n",
    "        q_words = q.split()\n",
    "        a_words = a.split()\n",
    "\n",
    "        # Check if any word in 'q' or 'a' is below the threshold\n",
    "        if any(voc.word_counts.get(word, 0) < threshold for word in q_words + a_words):\n",
    "            continue                                    # Skip this QA pair if any word is below the threshold\n",
    "\n",
    "        # Append the QA pair to the filtered list\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(qa_pairs), len(keep_pairs), len(keep_pairs) / len(qa_pairs)))\n",
    "    \n",
    "    return keep_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16a104be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 100847 pairs to 83359, 0.8266 of total\n"
     ]
    }
   ],
   "source": [
    "threshold = 3    # trial and error\n",
    "pairs = filter_by_word_frequency(voc, filtered_pairs, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e59e415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no no its my fault we didnt have a proper introduction', 'cameron .'],\n",
       " ['gosh if only we could find kat a boyfriend . . .',\n",
       "  'let me see what i can do .'],\n",
       " ['thats because its such a nice one .', 'forget french .'],\n",
       " ['there .', 'where ?'],\n",
       " ['you have my word . as a gentleman', 'youre sweet .']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6c3bc",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- voc = The WordIndexer Object instantiated, it contains the words of the dataset and corresponding indexes  \n",
    "- pairs = The question-answer pairs after all data preprocessing, in the form of [['q1', 'a1'], [  ], ......,[  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "663591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2indexes(voc, sentence):\n",
    "    \"\"\" Given a sentence as input and returns a list of indexes \n",
    "        corresponding to the words in the sentence,\n",
    "        followed by the EOS token.                               \n",
    "    Args: \n",
    "        voc : The WordIndexer class instantiated.\n",
    "        sentence : string : The sentence\n",
    "    returns:\n",
    "        list: the list of indices\n",
    "                                                                    \"\"\"\n",
    "    words = sentence.split()\n",
    "    indexes = [voc.get_word_index(word) for word in words]\n",
    "    indexes.append(EOS_token)\n",
    "    \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a7ac1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no no its my fault we didnt have a proper introduction'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76f90d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2indexes(voc, pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d655ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2],\n",
       " [15, 16, 17, 7, 18, 19, 20, 10, 21, 14, 14, 14, 2],\n",
       " [40, 41, 4, 42, 10, 43, 44, 14, 2],\n",
       " [59, 14, 2],\n",
       " [61, 9, 5, 62, 14, 63, 10, 64, 2],\n",
       " [66, 14, 2],\n",
       " [9, 80, 72, 34, 2],\n",
       " [26, 82, 34, 2]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST ON A BATCH_SIZE = 6\n",
    "batch_size = 8\n",
    "inp = []\n",
    "op = []\n",
    "for pair in pairs[:batch_size]:\n",
    "    inp.append(pair[0])\n",
    "    op.append(pair[1])\n",
    "    \n",
    "indexes = [sentence2indexes(voc, sentence) for sentence in inp]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d952a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad_rows(index_list, pad_token=PAD_token):\n",
    "    \"\"\"\n",
    "    Zero-pads the rows in the index_list so that all rows have the same length.\n",
    "    Transposes the resulting list of lists.\n",
    "\n",
    "    Args:\n",
    "        index_list (list[list[int]]): List of index lists to be zero-padded and transposed.\n",
    "        pad_token (int, optional): The padding token to use. Defaults to PAD_token = 0\n",
    "\n",
    "    Returns:\n",
    "        list[tuples]: Transposed and zero-padded list of tuples.\n",
    "                    Shape = ()\n",
    "\n",
    "    \"\"\"\n",
    "    max_length = max(len(row) for row in index_list)\n",
    "    \n",
    "    # Zero pad the rows\n",
    "    padded_list = [row + [pad_token] * (max_length - len(row)) for row in index_list]\n",
    "    \n",
    "    # Transpose the list[list]\n",
    "    transposed = list(zip(*padded_list))\n",
    "    \n",
    "    return transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88143313",
   "metadata": {},
   "source": [
    "### Testing the function - zero_pad_rows( ) and sentence2index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e352273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 15, 40, 59, 61, 66, 9, 26),\n",
       " (3, 16, 41, 14, 9, 14, 80, 82),\n",
       " (4, 17, 4, 2, 5, 2, 72, 34),\n",
       " (5, 7, 42, 0, 62, 0, 34, 2),\n",
       " (6, 18, 10, 0, 14, 0, 2, 0),\n",
       " (7, 19, 43, 0, 63, 0, 0, 0),\n",
       " (8, 20, 44, 0, 10, 0, 0, 0),\n",
       " (9, 10, 14, 0, 64, 0, 0, 0),\n",
       " (10, 21, 2, 0, 2, 0, 0, 0),\n",
       " (11, 14, 0, 0, 0, 0, 0, 0),\n",
       " (12, 14, 0, 0, 0, 0, 0, 0),\n",
       " (2, 14, 0, 0, 0, 0, 0, 0),\n",
       " (0, 2, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_list = zero_pad_rows(indexes)\n",
    "padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27c49c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is (max_length, batch_size) =  (13, 8)\n"
     ]
    }
   ],
   "source": [
    "rows = len(padded_list)\n",
    "columns = len(padded_list[0])\n",
    "\n",
    "print(\"Shape is (max_length, batch_size) = \", (rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d76b8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will later help us save space and time during training as it can be stored in 1 bit also\n",
    "def binaryMatrix(padded_list):\n",
    "    \"\"\" Given a padded matrix, converts it into a binary matrix\n",
    "    by replacing non-zero elements with 1, else 0\n",
    "    Args:\n",
    "        padded_list (list of tuples): A list of tuples representing a matrix.\n",
    "    Returns:\n",
    "        list[list]: The binary matrix .\n",
    "                                                                        \"\"\"\n",
    "    # convert non-zero elements to 1 and 0s to 0\n",
    "    binary_matrix = [[1 if element > 0 else 0 for element in row] for row in padded_list]\n",
    "    \n",
    "    return binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98f1acb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 0, 1, 0, 1, 1],\n",
       " [1, 1, 1, 0, 1, 0, 1, 0],\n",
       " [1, 1, 1, 0, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 1, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the function\n",
    "binaryMatrix(padded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab322c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fee97c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateInputTensor(word_indexer, sentence_list):\n",
    "    # Convert sentences to indexes\n",
    "    indexes_batch = [sentence2indexes(voc, sentence) for sentence in sentence_list]    \n",
    "    \n",
    "    # Get the lengths of each sentences + 1 (EOS_token)\n",
    "    lengths = torch.tensor([len(index) for index in indexes_batch])\n",
    "    \n",
    "    # Zero-pad the index list and transpose it, so as to be able to pass as batches\n",
    "    padded_batches = zero_pad_rows(indexes_batch)\n",
    "        \n",
    "    # Convert the transposed matrix to a LongTensor\n",
    "    input_tensor = torch.LongTensor(padded_batches)\n",
    "    \n",
    "    return input_tensor, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d054dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateOutputTensor(word_indexer, sentence_list):\n",
    "    # Convert sentences to indexes\n",
    "    indexes_batch = [sentence2indexes(voc, sentence) for sentence in sentence_list]    \n",
    "    \n",
    "    # Get the maximum target/output length in the batch\n",
    "    max_target_len = max([len(index) for index in indexes_batch])\n",
    "    \n",
    "    # Zero-pad the index list and transpose it, so as to be able to pass as batches\n",
    "    padded_batches = zero_pad_rows(indexes_batch)\n",
    "    \n",
    "    # Get the binary mask\n",
    "    binary_mask = binaryMatrix(padded_batches)\n",
    "    binary_mask = torch.ByteTensor(binary_mask)\n",
    "        \n",
    "    # Convert the transposed matrix to a LongTensor\n",
    "    output_tensor = torch.LongTensor(padded_batches)\n",
    "    \n",
    "    return output_tensor, binary_mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac604046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2Train(word_indexer, qa_batches):\n",
    "    \"\"\"\n",
    "    Convert question-answer batches into input and output tensors for training.\n",
    "\n",
    "    Arguments:\n",
    "        word_indexer (WordIndexer): An instance of the WordIndexer class.\n",
    "        qa_batches (list of lists): A list of question-answer batches, where each batch is a list of two elements: question and answer.\n",
    "\n",
    "    Returns:\n",
    "        input_tensor (torch.LongTensor): The input tensor containing the indexes of the questions after padding.\n",
    "        input_lengths (torch.Tensor): The tensor containing the lengths of each input sequence.\n",
    "        output_tensor (torch.LongTensor): The output tensor containing the indexes of the answers after padding.\n",
    "        output_mask (torch.ByteTensor): The binary mask indicating the positions with non-zero elements in the output tensor.\n",
    "        max_target_len (int): The maximum length of the target/output sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort the batches in descending order of question length as in number of words\n",
    "    sorted_batches = sorted(qa_batches, key=lambda x: len(x[0].split()), reverse=True)\n",
    "    \n",
    "    question_batch, answer_batch = [], []\n",
    "    for qa_batch in sorted_batches:\n",
    "        question_batch.append(qa_batch[0])\n",
    "        answer_batch.append(qa_batch[1])\n",
    "        \n",
    "    # generate input tensor and input lengths\n",
    "    input_tensor, input_lengths = generateInputTensor(word_indexer, question_batch)\n",
    "    \n",
    "    # get output tensor, binary mask, and max target length\n",
    "    output_tensor, output_mask, max_target_len = generateOutputTensor(word_indexer, answer_batch)\n",
    "    \n",
    "    return input_tensor, input_lengths, output_tensor, output_mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8193c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f265b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Visualising pairs[0:2] ------------\n",
      "[['no no its my fault we didnt have a proper introduction', 'cameron .'], ['gosh if only we could find kat a boyfriend . . .', 'let me see what i can do .']]\n",
      "\n",
      "\n",
      "------------------ Printing qa_batches --------------- \n",
      " [['i know .', 'how do you know ?'], ['home .', 'someone waiting for you ?'], ['thank god .', 'first time i ever heard that .'], ['how about someone who really really liked shostakovich ?', 'are you asking me to marry you ?'], ['you dont like the tune find another station .', 'what are you hiding danny ?'], ['i am arthur king of the britons .', 'my liege . . . forgive me . . .']]\n",
      "\n",
      "\n",
      "---------- VALIDATING THE ABOVE FUNCTIONS DEFINED ---------------\n",
      "Input tensor\n",
      "tensor([[   47,    61,    26,    26,   605,   137],\n",
      "        [  125,   141,   613,    74,   225,    14],\n",
      "        [   55,    68,  4481,    14,    14,     2],\n",
      "        [  130,    38,  2391,     2,     2,     0],\n",
      "        [  409,  2285,   106,     0,     0,     0],\n",
      "        [  409,    19,    38,     0,     0,     0],\n",
      "        [ 1163,   833, 11571,     0,     0,     0],\n",
      "        [ 9915,   767,    14,     0,     0,     0],\n",
      "        [   34,    14,     2,     0,     0,     0],\n",
      "        [    2,     2,     0,     0,     0,     0]])\n",
      "Input lengths: tensor([10, 10,  9,  4,  4,  3])\n",
      "Output tensor\n",
      "tensor([[  184,    25,     5,    47,   653,    55],\n",
      "        [   61,   184, 11572,    28,   350,  1121],\n",
      "        [  606,    61,    14,    61,    26,    37],\n",
      "        [   23,  5748,    14,    74,   494,    61],\n",
      "        [   85,  2897,    14,    34,   182,    34],\n",
      "        [ 3063,    34,   615,     2,   170,     2],\n",
      "        [   61,     2,    23,     0,    14,     0],\n",
      "        [   34,     0,    14,     0,     2,     0],\n",
      "        [    2,     0,    14,     0,     0,     0],\n",
      "        [    0,     0,    14,     0,     0,     0],\n",
      "        [    0,     0,     2,     0,     0,     0]])\n",
      "Binary output/target mask\n",
      "tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "Max target length:  11\n"
     ]
    }
   ],
   "source": [
    "# Small Example run\n",
    "example_batch_size = 6\n",
    "print(\"--------------- Visualising pairs[0:2] ------------\")\n",
    "print(pairs[:2])\n",
    "print(\"\\n\")\n",
    "qa_batches = [random.choice(pairs) for _ in range(example_batch_size)]\n",
    "print(\"------------------ Printing qa_batches --------------- \\n\", qa_batches)\n",
    "print(\"\\n\")\n",
    "\n",
    "batches = batch2Train(voc, qa_batches)\n",
    "input_tensor, input_lengths, output_tensor, output_mask, max_target_len = batches  # unpack the tuple\n",
    "\n",
    "print(\"---------- VALIDATING THE ABOVE FUNCTIONS DEFINED ---------------\")\n",
    "print(\"Input tensor\")\n",
    "print(input_tensor)\n",
    "print(\"Input lengths:\", input_lengths)\n",
    "\n",
    "print(\"Output tensor\")\n",
    "print(output_tensor)\n",
    "\n",
    "print(\"Binary output/target mask\")\n",
    "print(output_mask)\n",
    "\n",
    "print(\"Max target length: \", max_target_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913229a",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c244a07",
   "metadata": {},
   "source": [
    "The model is built using these 2 blocks, which is discussed briefly below\n",
    "### Encoder\n",
    "-  aedewfwefw\n",
    "\n",
    "### Decoder\n",
    "- wefewfwefw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ceb22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5105a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6ae16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46766752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d429f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225815c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- edkjewbfowen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe2bf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, voc , hidden_size, n_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder RNN module for sequence encoding using GRU.\n",
    "\n",
    "        Args:\n",
    "            voc (Vocabulary): The Vocabulary object containing relevant information like num_words\n",
    "            hidden_size (int): Size of GRU's hidden state \n",
    "            n_layers (int): Number of GRU layers. Defaults to 1.\n",
    "            dropout (float): Dropout probability between GRU layers.set to be 0, if n_layers = 1\n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = voc.num_words\n",
    "\n",
    "        # Initialize word embeddings \n",
    "        self.embedding = nn.Embedding(self.input_size, hidden_size)\n",
    "        \n",
    "        # Input size of the GRU is set to hidden size because our \n",
    "        # input word is a word embedding with num of features = hidden_size (it can be anything though)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=n_layers, bidirectional=True, \n",
    "                          dropout=(0 if n_layers == 1 else dropout))\n",
    "\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderRNN.\n",
    "\n",
    "        Args:\n",
    "            input_seq (torch.Tensor): Input sequence of word indexes. Shape: (max_length, batch_size).\n",
    "            input_lengths (list): List of sequence lengths in the batch.\n",
    "            hidden (torch.Tensor, optional): Initial hidden state of the GRU. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Outputs : hidden states at each time steps of the GRU. Shape: (max_len, batch_size, hidden_size * 2).\n",
    "            hidden_state: GRU's Final hidden state. Shape: (num_layers * num_directions, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Word indexes to word embeddings\n",
    "        embedded = self.embedding(input_seq)  # embedded shape: (sequence_length, batch_size, hidden_size)\n",
    "        # Pack padded batch of sequences for RNN module, used to handle variable input lengths\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "\n",
    "        outputs, hidden_state = self.gru(packed, hidden_state)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)  # outputs shape: (sequence_length, batch_size, hidden_size * 2)\n",
    "\n",
    "        # Sum the forward and backward hidden states of the bidirectional GRU\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        \n",
    "        # outputs: the output features h_t from the last layer of the GRU, at each timestep\n",
    "        return outputs, hidden_state          # hidden_state from the last tine step(hn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42bc6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=EncoderRNN(voc , 300, n_layers=1, dropout=0)\n",
    "ans=encoder.forward(input_tensor, input_lengths=input_lengths, hidden_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edaf6d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 300])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d9273",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d218fe2",
   "metadata": {},
   "source": [
    "- This code for the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85849415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention:\n",
    "# encoder outputs \n",
    "# Method 1, using score as dot product between encoder's outputs and decoder's hidden state\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def forward(self,encoderOutputs,hidden):\n",
    "        return torch.sum(hidden * encoderOutputs,dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e51a7",
   "metadata": {},
   "source": [
    "**Decoder RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4bd99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder => uses attention b/w encoder's outputs and decoder's hidden state\n",
    "class decoderRNN(nn.Module):\n",
    "    # INFO:\n",
    "    # 1. outputSize : It contains value as voc.num_words\n",
    "    # 2. hidden : It contains value as last hidden state of encoder (2 x 6 x 300)\n",
    "    # 3. hidden_size : It is equal to 300 => size of the vector that we are making\n",
    "    # 4. voc : It is the complete vocabulary\n",
    "    def __init__(self,voc,hidden_size,n_layers,batch_size, dropout = 0.1):\n",
    "        super(decoderRNN,self).__init__()\n",
    "        self.input_size =voc.num_words\n",
    "        self.outputSize= voc.num_words\n",
    "        self.hidden_size=hidden_size\n",
    "        self.dropout=dropout\n",
    "        self.n_layers=n_layers\n",
    "        self.batch_size=batch_size\n",
    "        # 1.\n",
    "        # Initialize word embeddings \n",
    "        # This function comverts the input (1,batch_size)[1st time step] to a vector of dimetion 300 based on voc indexes\n",
    "        self.embedding = nn.Embedding(self.input_size, hidden_size) # embedded shape: (batch_size, hidden_size)\n",
    "        # after embedding, we get size as 6 x 300\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        # 2.\n",
    "        #  Now we do the GRU stuff\n",
    "        #  We output both the output and the hidden_state, hidden state we further use for finding attention\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=n_layers, \n",
    "                          dropout=(0 if n_layers == 1 else dropout))\n",
    "        # 3.\n",
    "        # Now the attention\n",
    "        # A  simple attention based on dot product between encoder's outputs and hidden state of decoder\n",
    "        self.attn = Attention()\n",
    "        \n",
    "        # \n",
    "        self.FFNN1=nn.Linear(16,self.input_size)\n",
    "\n",
    "        self.FFNN2=nn.Linear(self.input_size,self.outputSize)\n",
    "    # variables:\n",
    "\n",
    "    # 1.inputData : this a single time step , dim of 1 x 6\n",
    "    # 2. encoderLastHidden : this is the encoder's last hidden state , dim of 2 x 6 x 300\n",
    "    # encoderOutputs\n",
    "    # encoderOutputs\n",
    "    def forward(self,inputData,encoderLastHidden,encoderOutputs):\n",
    "        # step 1 -  here u embed the 1 x batch_size [1,213,23,4,44,432] to a batch_size x hidden(300) vectors\n",
    "        embedding = self.embedding(inputData)\n",
    "        embedding = self.embedding_dropout(embedding)\n",
    "        # step 2  - GRU thing\n",
    "        decoderOutput, hiddenState = self.gru(embedding, encoderLastHidden[0])\n",
    "\n",
    "        # step 3 - Now apply the attention to the encoder outputs and hidden state\n",
    "        attentionWeights=self.attn.forward(decoderOutput,encoderOutputs)\n",
    "        #print(attentionWeights.size(),encoderOutputs.size())\n",
    "        # step 4 - multply the wiegts and encoder outputs, this gives context\n",
    "        context=attentionWeights.unsqueeze(1).bmm(encoderOutputs)\n",
    "        context=context.transpose(0,1)[0].transpose(0,1)\n",
    "        # step 5 - here we have the decoderOutput of dim 300 and also we concatenate contex to this amking a 2* self.input_size to make a vector cantaining the context and output\n",
    "        # Now, make a FFNN between 2* input_size and inputsize\n",
    "      \n",
    "        ConcatedVector_input = torch.cat((decoderOutput.transpose(0,1), context), 1)\n",
    "        ConcatedVector_output = torch.tanh(self.FFNN1(ConcatedVector_input))\n",
    "        \n",
    "        # step 6 - Finally, make a FFNN between 2* input_size and inputsize\n",
    "        # finalOutput is of dimension voc.num_words\n",
    "        finalOutput=self.FFNN2(ConcatedVector_output)\n",
    "        \n",
    "        # step 7 - apply softmax \n",
    "        finalOutput=F.softmax(finalOutput, dim=1)\n",
    "\n",
    "        # now return the hidden stata and output\n",
    "        return finalOutput,hiddenState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e8971",
   "metadata": {},
   "source": [
    "*decoder run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff228f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.0916e-05, 6.1009e-05, 1.4920e-04,  ..., 1.0211e-04, 7.0448e-05,\n",
       "          3.9080e-05],\n",
       "         [5.5520e-05, 6.0984e-05, 1.0417e-04,  ..., 7.4741e-05, 5.6895e-05,\n",
       "          4.1878e-05],\n",
       "         [1.0117e-04, 6.9762e-05, 7.2131e-05,  ..., 7.4872e-05, 4.9256e-05,\n",
       "          9.1082e-05],\n",
       "         ...,\n",
       "         [4.9443e-05, 7.6325e-05, 1.0631e-04,  ..., 8.6037e-05, 8.0963e-05,\n",
       "          5.2117e-05],\n",
       "         [4.3179e-05, 7.5870e-05, 1.0020e-04,  ..., 8.7553e-05, 7.2238e-05,\n",
       "          5.9592e-05],\n",
       "         [1.0162e-04, 6.6477e-05, 7.9408e-05,  ..., 4.2883e-05, 5.7729e-05,\n",
       "          8.8069e-05]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.0577, -0.5455,  0.5709,  ..., -0.1032, -0.0665, -0.2428],\n",
       "         [ 0.0377,  0.2740,  0.0081,  ..., -0.0994,  0.0461,  0.0799],\n",
       "         [-0.1138, -0.0840, -0.0678,  ...,  0.1221,  0.0186, -0.0135],\n",
       "         [-0.0096, -0.0528,  0.0030,  ...,  0.0310,  0.0209, -0.0273],\n",
       "         [-0.1412,  0.0401,  0.0836,  ..., -0.0111,  0.0143, -0.0152],\n",
       "         [-0.0046, -0.0034,  0.0394,  ...,  0.0806, -0.0695,  0.0900]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create an instance of decoder\n",
    "mydecoder= decoderRNN(voc,hidden_size=300,dropout=0,n_layers=6,batch_size=6)\n",
    "decoderOutput=mydecoder.forward(input_tensor[0],encoderLastHidden=ans[1],encoderOutputs=ans[0])\n",
    "decoderOutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8381d02",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f0ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
